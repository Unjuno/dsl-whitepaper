# モデルの応答速度：モデル内部の仕組みから算出

## 結論

**応答速度は「1トークンを出すのに必要な理論計算量 × 出力トークン数」**でほぼ決まります。

## 1. 応答速度を決めるモデル内部要因

### 1.1 基本式（抽象化）

自己回帰型LLM（GPT系）の生成時間は次で近似できます：

T_total ≈ Σ_{t=1}^{L_out} T_token(t)

ここで：
- L_out：出力トークン数
- T_token(t)：t番目トークン生成に必要な内部計算量

### 1.2 1トークン生成の内部構造

1トークン生成は以下で構成されます：

T_token ~ L × (T_Attention + T_FFN)

#### (A) 層数 L

- Transformer Block の段数
- 線形に効く
- 例：32層 → 64層で理論的に2倍遅くなる

#### (B) 自己注意（Self-Attention）

T_Attention ~ O(n·d)

- n：文脈長（これまでのトークン数）
- d：隠れ次元

⚠️ **重要**：生成の後半ほど n が増えるため、同じモデルでも後半トークンほど遅くなる。

KV-cache があっても「過去を再計算しない」だけで「過去を参照するコスト」は残る。

#### (C) FFN（Feed Forward Network）

T_FFN ~ O(d·d_ff)

- 文脈長に依存しない
- モデルサイズ依存の定数時間
- 👉 大規模モデルほど FFN が支配的になることが多い

### 1.3 隠れ次元とヘッド数

| 要素 | 影響 |
|------|------|
| 隠れ次元 d | Attention / FFN 両方に効く |
| ヘッド数 h | 理論的には d が一定なら同等 |
| FFN拡張率 | d_ff = 4d など → 速度に直撃 |

### 1.4 出力トークン数（最重要）

T_total ∝ L_out

- 長文出力 = 必ず遅い
- ストリーミングは「速く見える」だけで総計算量は同じ

### 1.5 トークナイザの粒度（地味だが効く）

同じ文章でも：
- 英語：少トークン
- 日本語：多トークン

トークン数が2倍 → 時間もほぼ2倍

### 1.6 サンプリング方式（logit処理）

| 手法 | 理論的影響 |
|------|-----------|
| Greedy | 最小 |
| Top-k | O(k) |
| Top-p | ソート・累積和が増える |
| Temperature | 無視できる |

※ 通常は Attention / FFN に比べると微小

## 2. 応用・転用例

### 2.1 応答が遅い原因の切り分け

| 症状 | モデル内部要因 |
|------|---------------|
| 最初は速いが後半遅い | 文脈長 n 増大 |
| 短文でも遅い | 層数・隠れ次元 |
| 日本語が遅い | トークン分割数 |
| 出力を止めると速い | 自己回帰生成 |

### 2.2 速度を上げたいなら（HW抜き）

- 出力トークンを減らす
- コンテキストを削る
- 層数が少ないモデルを使う
- FFNが軽い設計（Mixture-of-Experts など）

## 3. 特化モデルの方が速くなる理由

「特化モデルの方が速くなる」ことは理論的にも実際にも起こります。
ただしそれは魔法ではなく、計算経路が削られているだけです。

### 本質的理由（仕組みベース）

汎用LLMは、常に次を想定して計算します：
- あらゆる話題
- あらゆる文体
- あらゆる推論深度

特化モデルは、特定の領域に絞ることで：
- 不要な計算経路を削減
- より効率的な推論が可能

## 4. 事実／推論／仮説の区分

### 事実
- Transformer の計算量構造
- 自己回帰生成の逐次性

### 推論
- レイテンシはほぼトークン数比例

### 仮説
- MoE や線形注意が主流化すれば後半遅延は緩和される

## 5. 学際的関連

- **計算量理論**：逐次計算と並列不能性
- **情報理論**：トークン化＝符号長問題
- **制御工学**：逐次推定（フィルタ）構造
- **経済学**：限界コストが一定な生産モデル
